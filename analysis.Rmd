---
title: "Analysis of Workplace-Themed Term Co-Occurrence Matrices"
author: "Ben Kubacki"
date: "20 November 2024"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="")

library(tidyverse)
library(readxl)

# web scraping
library(rvest)

# tokenizer
library(tidytext)

# word embeddings
library(text2vec)
library(Rling)
library(cluster)
library(tm)
```

# Load packages in hidden setup chunk (see list below) 

```{r}

# library(tidyverse)
# library(readxl)
# 
# # web scraping
# library(rvest)
# 
# # tokenizer
# library(tidytext)
# 
# # word embeddings
# library(text2vec)
# library(Rling)
# library(cluster)
# library(tm)

```

# Read in data

- Files: 22  
- Type: text  
- Contents: Document number, text  
- Text components: spaces, `#` paragraph or sentence separators  

Method  

- Assign to object `news_raw`  
- Use `c()` to combine all relative paths of the files as strings in character vector  
- Read in using `read_lines()`  
- Use `map()` to iterate `read_lines()` over all 22 files at once  
- Use `unlist()` to turn from list containing character vectors (each doc's text) into character vector containing elements (each doc's text)  
- Read in from `data_private/` folder within this repo but ignored with `.gitignore`  

```{r}
news <- c("data_private/text_newspaper_lsp/w_news_1990.txt",
           "data_private/text_newspaper_lsp/w_news_1991.txt",
           "data_private/text_newspaper_lsp/w_news_1992.txt",
           "data_private/text_newspaper_lsp/w_news_1993.txt",
           "data_private/text_newspaper_lsp/w_news_1994.txt",
           "data_private/text_newspaper_lsp/w_news_1995.txt",
           "data_private/text_newspaper_lsp/w_news_1996.txt",
           "data_private/text_newspaper_lsp/w_news_1997.txt",
           "data_private/text_newspaper_lsp/w_news_1998.txt",
           "data_private/text_newspaper_lsp/w_news_1999.txt",
           "data_private/text_newspaper_lsp/w_news_2000.txt",
           "data_private/text_newspaper_lsp/w_news_2001.txt",
           "data_private/text_newspaper_lsp/w_news_2002.txt",
           "data_private/text_newspaper_lsp/w_news_2003.txt",
           "data_private/text_newspaper_lsp/w_news_2004.txt",
           "data_private/text_newspaper_lsp/w_news_2005.txt",
           "data_private/text_newspaper_lsp/w_news_2006.txt",
           "data_private/text_newspaper_lsp/w_news_2007.txt",
           "data_private/text_newspaper_lsp/w_news_2008.txt",
           "data_private/text_newspaper_lsp/w_news_2009.txt",
           "data_private/text_newspaper_lsp/w_news_2010.txt",
           "data_private/text_newspaper_lsp/w_news_2011.txt",
           "data_private/text_newspaper_lsp/w_news_2012.txt") %>% 
  map(read_lines) %>% unlist() 

```

Create data frame + Rename

- Create two columns and then separate using `str_sub()`

```{r}
news <- tibble(news)

news <- news %>% 
  rename(document_id = news)

news <- news %>% 
  select(document_id = document_id, text = document_id)

news <- news %>% mutate(document_id = str_sub(document_id, 3,9), text = str_sub(text, start = 10))
```

Note: There is an irregularity in the way the input documents (i.e., 1990 vs. 2012) represent the document numbers and paragraph breaks.

- head(news) yields "3000002" while tail(news) yields 4115379 without quotation marks. Both are character strings.  
- head(news) prints 

```{r}

# news %>% 
#   head() %>% 
#   str_split("\\s+") %>% 
#   length(news$text)

tail(news)

```


# Create term-co-occurrence matrix and find cosine similarities

Source of demo, including object names, code, and comments inside code: (Selivanov, 2023)[https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html]

"You can achieve much better results by experimenting with skip_grams_window and the parameters of the GloVe class (including word vectors size and the number of iterations)."

```{r}
# Create iterator over tokens
tokens <- space_tokenizer(news$text)

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
```

Use source's vocab list first, change later to mine.

Term count min: Minimum number of token counts for a term to be included in the corpus.

**Set at 2**

Use arguments to determine term and doc count mins and maxes, as well as overall vocab max.


```{r}

vocab <- prune_vocabulary(vocab, term_count_min = 2L)

```


Skip Gram Window: Size of context window around the target word

**Set at 5**. Consider 10...


```{r}
# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)
```


```{r}
# use window of 5 for context words

tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

## Include all this code together so I can run it all overnight.

glove <- GlobalVectors$new(rank = 50, x_max = 10)

wv_main <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)

wv_context <- glove$components

word_vectors <- wv_main + t(wv_context)

cos_list <- sim2(x = word_vectors, y = word_vectors["work", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T) 

cos_tbl <- cos_list %>% 
  as.tibble(rownames = "words") %>% 
  gather("value", key = "value2", value = value) %>% 
  select(words = words, cos_sim = value)

# write_csv(cos_tbl, "data_private/cosine_work.csv")

```


May need to specify the number of cores, according to author.

```{r}
# glove = GlobalVectors$new(rank = 50, x_max = 10)
# wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
```


```{r}
# wv_context = glove$components
# word_vectors = wv_main + t(wv_context)
```

```{r}
# test <- word_vectors["meeting", , drop = FALSE] -
#   word_vectors["meet", , drop = FALSE] +
#   word_vectors["conference", , drop = FALSE]
# cos_sim = sim2(x = word_vectors, y = test, method = "cosine", norm = "l2")
# head(sort(cos_sim[,1], decreasing = TRUE), 5)

# by <- word_vectors["and", , drop = FALSE] -
#   word_vectors["to", , drop = FALSE] +
#   word_vectors["with", , drop = FALSE]
# cos_sim = sim2(x = word_vectors, y = by, method = "cosine", norm = "l2")
# head(sort(cos_sim[,1], decreasing = TRUE), 5)

## Get full list of cosine similarities in order, according to target word
# sim2(x = word_vectors, y = word_vectors["cheese", , drop=FALSE], method = "cosine", norm = "l2")[,1] %>% sort(decreasing = T) %>% head(5)


```

Analyze the results

- 393,667 words (remember the `prune_vocab()` function removed words)  
- top words "Instead" (0.8731426), "doing" (0.8704069), "time" (0.8649227), "way" (0.8642288), "job" (0.8560277), "same" (0.8524236), "done" (0.8522326), "now" (0.8521537), "rather" (0.8518153), "own" (0.8360136), "well" (0.8346018).

```{r}

cos_tbl %>% summary() ## 393,667 words

cos_tbl %>% slice(2:21)

```

Visualize?

```{r}

mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = author, values_from = proportion) %>%
  pivot_longer(`BrontÃ« Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

arrange(proportion)

ggplot(slice(cos_tbl, 1:50), aes(x = words, y = cos_sim)) + geom_col()

```



Add K-Bands

```{r}

output_words <- 
  cos_tbl$words

output_words %>% 
  summary()

output_words_sample <- 
  output_words[1:200] %>% writeLines()

k_bands_1 <- c("about_[1] actually_[1] addition_[1] all_[1] alone_[1] already_[1] also_[1] always_[1] and_[2] as_[2] be_[1] because_[2] better_[1] both_[1] bring_[1] bringing_[1] brought_[1] business_[1] but_[2] can_[1] come_[1] comes_[1] coming_[1] could_[1] course_[1] did_[1] different_[1] do_[1] does_[1] doing_[1] done_[1] either_[1] even_[2] every_[1] everyone_[1] everything_[1] experience_[1] find_[1] finding_[1] for_[2] get_[1] giving_[1] go_[1] going_[1] good_[1] hard_[1] have_[1] having_[1] he_[2] help_[1] helping_[1] here_[1] how_[1] i_[1] idea_[1] if_[2] instead_[2] is_[1] it_[2] job_[1] just_[1] keep_[1] kids_[1] kind_[1] learn_[1] learned_[1] learning_[1] life_[1] like_[1] long_[1] look_[1] looking_[1] lot_[1] made_[1] make_[1] makes_[1] making_[1] may_[1] means_[1] meant_[1] might_[1] more_[1] move_[1] much_[1] must_[1] need_[1] needed_[1] needs_[1] new_[1] not_[2] now_[2] often_[1] once_[2] one_[1] only_[1] or_[1] other_[1] others_[1] our_[1] own_[1] part_[1] people_[1] place_[1] planning_[1] program_[1] put_[1] putting_[1] rather_[1] re_[1] really_[1] rest_[1] same_[1] see_[1] set_[1] she_[2] should_[1] show_[1] simply_[1] so_[2] some_[1] something_[1] sometimes_[1] sort_[1] special_[1] start_[1] still_[1] such_[1] supposed_[1] take_[1] taken_[1] takes_[1] taking_[1] that_[2] their_[1] them_[1] there_[2] these_[1] they_[2] things_[1] think_[1] this_[1] those_[1] though_[1] thought_[1] time_[1] to_[1] took_[1] trying_[1] us_[1] used_[1] usually_[1] ve_[1] wanted_[1] way_[1] ways_[1] we_[2] well_[1] what_[1] when_[2] which_[1] while_[2] will_[1] without_[1] work_[1] worked_[1] working_[1] works_[1] would_[1] years_[1] yet_[1] you_[1]")

k_bands_1 <- k_bands_1 %>% 
  tibble(word = `.`, k_band = k_bands_1, number = k_bands_1) 

k_bands_1$word <- k_bands_1$word %>% 
  # unnest_tokens(word, word, token = "", format = "text", drop = TRUE)
  str_split("\\s+")

k_bands_1
```

Need to remove stop words and lemmatize.

Stop words:

```{r}

cos_tbl %>% 
  filter(!words %in% stopwords(kind = "en")) %>% 
  slice(2:21)

cos_tbl_filtered <- cos_tbl %>% 
  filter(!words %in% stopwords(kind = "en"))

```

Lemmatize: 

# Session info

```{r}
sessionInfo()
```
