---
title: "Analysis of Workplace-Themed Term Co-Occurrence Matrices"
author: "Ben Kubacki"
date: "20 November 2024"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="")

library(tidyverse)
library(readxl)

# web scraping
library(rvest)

# tokenizer
library(tidytext)

# word embeddings
library(text2vec)
library(Rling)
library(cluster)
library(tm)
```

# Load packages in hidden setup chunk (see list below) 

```{r}

# library(tidyverse)
# library(readxl)
# 
# # web scraping
# library(rvest)
# 
# # tokenizer
# library(tidytext)
# 
# # word embeddings
# library(text2vec)
# library(Rling)
# library(cluster)
# library(tm)

```

# Read in data

- Files: 22  
- Type: text  
- Contents: Document number, text  
- Text components: spaces, `#` paragraph or sentence separators  

Method  

- Assign to object `news_raw`  
- Use `c()` to combine all relative paths of the files as strings in character vector  
- Read in using `read_lines()`  
- Use `map()` to iterate `read_lines()` over all 22 files at once  
- Use `unlist()` to turn from list containing character vectors (each doc's text) into character vector containing elements (each doc's text)  
- Read in from `data_private/` folder within this repo but ignored with `.gitignore`  

```{r}
news <- c("data_private/text_newspaper_lsp/w_news_1990.txt",
           "data_private/text_newspaper_lsp/w_news_1991.txt",
           "data_private/text_newspaper_lsp/w_news_1992.txt",
           "data_private/text_newspaper_lsp/w_news_1993.txt",
           "data_private/text_newspaper_lsp/w_news_1994.txt",
           "data_private/text_newspaper_lsp/w_news_1995.txt",
           "data_private/text_newspaper_lsp/w_news_1996.txt",
           "data_private/text_newspaper_lsp/w_news_1997.txt",
           "data_private/text_newspaper_lsp/w_news_1998.txt",
           "data_private/text_newspaper_lsp/w_news_1999.txt",
           "data_private/text_newspaper_lsp/w_news_2000.txt",
           "data_private/text_newspaper_lsp/w_news_2001.txt",
           "data_private/text_newspaper_lsp/w_news_2002.txt",
           "data_private/text_newspaper_lsp/w_news_2003.txt",
           "data_private/text_newspaper_lsp/w_news_2004.txt",
           "data_private/text_newspaper_lsp/w_news_2005.txt",
           "data_private/text_newspaper_lsp/w_news_2006.txt",
           "data_private/text_newspaper_lsp/w_news_2007.txt",
           "data_private/text_newspaper_lsp/w_news_2008.txt",
           "data_private/text_newspaper_lsp/w_news_2009.txt",
           "data_private/text_newspaper_lsp/w_news_2010.txt",
           "data_private/text_newspaper_lsp/w_news_2011.txt",
           "data_private/text_newspaper_lsp/w_news_2012.txt") %>% 
  map(read_lines) %>% unlist() 

```

Create data frame + Rename

- Create two columns and then separate using `str_sub()`

```{r}
news <- tibble(news)

news <- news %>% 
  rename(document_id = news)

news <- news %>% 
  select(document_id = document_id, text = document_id)

news <- news %>% mutate(document_id = str_sub(document_id, 3,9), text = str_sub(text, start = 10))
```

Note: There is an irregularity in the way the input documents (i.e., 1990 vs. 2012) represent the document numbers and paragraph breaks.

- head(news) yields "3000002" while tail(news) yields 4115379 without quotation marks. Both are character strings.  
- head(news) prints 

```{r}

# news %>% 
#   head() %>% 
#   str_split("\\s+") %>% 
#   length(news$text)

tail(news)

```


# Create term-co-occurrence matrix and find cosine similarities

Source of demo, including object names, code, and comments inside code: (Selivanov, 2023)[https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html]

"You can achieve much better results by experimenting with skip_grams_window and the parameters of the GloVe class (including word vectors size and the number of iterations)."

```{r}
# Create iterator over tokens
tokens <- space_tokenizer(news$text)

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
```

Use source's vocab list first, change later to mine.

Term count min: Minimum number of token counts for a term to be included in the corpus.

**Set at 2**

Use arguments to determine term and doc count mins and maxes, as well as overall vocab max.


```{r}

vocab <- prune_vocabulary(vocab, term_count_min = 2L)

```


Skip Gram Window: Size of context window around the target word

**Set at 5**. Consider 10...


```{r}
# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)
```


```{r}
# use window of 5 for context words

tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

## Include all this code together so I can run it all overnight.

glove <- GlobalVectors$new(rank = 50, x_max = 10)

wv_main <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)

wv_context <- glove$components

word_vectors <- wv_main + t(wv_context)

cos_list <- sim2(x = word_vectors, y = word_vectors["work", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T) 

cos_tbl <- cos_list %>% 
  as.tibble(rownames = "words") %>% 
  gather("value", key = "value2", value = value) %>% 
  select(words = words, cos_sim = value) 

write_csv(cos_tbl, "data_private/cosine_work.csv")

```

May need to specify the number of cores, according to author.

```{r}
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
```


```{r}
wv_context = glove$components
word_vectors = wv_main + t(wv_context)
```

```{r}
# test <- word_vectors["meeting", , drop = FALSE] -
#   word_vectors["meet", , drop = FALSE] +
#   word_vectors["conference", , drop = FALSE]
# cos_sim = sim2(x = word_vectors, y = test, method = "cosine", norm = "l2")
# head(sort(cos_sim[,1], decreasing = TRUE), 5)


## Get full list of cosine similarities in order, according to target word
sim2(x = word_vectors, y = word_vectors["cheese", , drop=FALSE], method = "cosine", norm = "l2")[,1] %>% sort(decreasing = T) %>% head(5)


# by <- word_vectors["and", , drop = FALSE] -
#   word_vectors["to", , drop = FALSE] +
#   word_vectors["with", , drop = FALSE]
# cos_sim = sim2(x = word_vectors, y = by, method = "cosine", norm = "l2")
# head(sort(cos_sim[,1], decreasing = TRUE), 5)
```



# Session info

```{r}
sessionInfo()
```
