---
title: "Analysis of Workplace-Themed Term Co-Occurrence Matrices"
author: "Ben Kubacki"
date: "20 November 2024"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="")

library(tidyverse)
library(tidyr)
library(dplyr)
library(purrr)
library(stringr)
library(readxl)

# web scraping
library(rvest)

# tokenizer
library(tidytext)

# word embeddings
library(text2vec)
library(Rling)
library(cluster)
library(tm)
```

# Load packages in hidden setup chunk (see list below) 

```{r}

# library(tidyverse)
# library(dplyr)
# library(purrr)
# library(stringr)
# library(readxl)
# 
# # web scraping
# library(rvest)
# 
# # tokenizer
# library(tidytext)
# 
# # word embeddings
# library(text2vec)
# library(Rling)
# library(cluster)
# library(tm)

```

# Read in data

- Files: 22  
- Type: text  
- Contents: Document number, text  
- Text components: spaces, `#` paragraph or sentence separators  

Method  

- Assign to object `news_raw`  
- Use `c()` to combine all relative paths of the files as strings in character vector  
- Read in using `read_lines()`  
- Use `map()` to iterate `read_lines()` over all 22 files at once  
- Use `unlist()` to turn from list containing character vectors (each doc's text) into character vector containing elements (each doc's text)  
- Read in from `data_private/` folder within this repo but ignored with `.gitignore`  

```{r}
news <- c("data_private/text_newspaper_lsp/w_news_1990.txt",
           "data_private/text_newspaper_lsp/w_news_1991.txt",
           "data_private/text_newspaper_lsp/w_news_1992.txt",
           "data_private/text_newspaper_lsp/w_news_1993.txt",
           "data_private/text_newspaper_lsp/w_news_1994.txt",
           "data_private/text_newspaper_lsp/w_news_1995.txt",
           "data_private/text_newspaper_lsp/w_news_1996.txt",
           "data_private/text_newspaper_lsp/w_news_1997.txt",
           "data_private/text_newspaper_lsp/w_news_1998.txt",
           "data_private/text_newspaper_lsp/w_news_1999.txt",
           "data_private/text_newspaper_lsp/w_news_2000.txt",
           "data_private/text_newspaper_lsp/w_news_2001.txt",
           "data_private/text_newspaper_lsp/w_news_2002.txt",
           "data_private/text_newspaper_lsp/w_news_2003.txt",
           "data_private/text_newspaper_lsp/w_news_2004.txt",
           "data_private/text_newspaper_lsp/w_news_2005.txt",
           "data_private/text_newspaper_lsp/w_news_2006.txt",
           "data_private/text_newspaper_lsp/w_news_2007.txt",
           "data_private/text_newspaper_lsp/w_news_2008.txt",
           "data_private/text_newspaper_lsp/w_news_2009.txt",
           "data_private/text_newspaper_lsp/w_news_2010.txt",
           "data_private/text_newspaper_lsp/w_news_2011.txt",
           "data_private/text_newspaper_lsp/w_news_2012.txt") %>% 
  map(read_lines) %>% unlist() 

```

Create data frame + Rename

- Create two columns and then separate using `str_sub()`

```{r}
news <- tibble(news)

news <- news %>% 
  rename(document_id = news)

news <- news %>% 
  select(document_id = document_id, text = document_id)

news <- news %>% mutate(document_id = str_sub(document_id, 3,9), text = str_sub(text, start = 10)) 

news %>% tail()

```


# Create term-co-occurrence matrix and find cosine similarities

Source of demo, including object names, code, and comments inside code: (Selivanov, 2023)[https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html]

"You can achieve much better results by experimenting with skip_grams_window and the parameters of the GloVe class (including word vectors size and the number of iterations)."

```{r}
# Create iterator over tokens
tokens <- space_tokenizer(news$text)

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
```

Use source's vocab list first, change later to mine.

Term count min: Minimum number of token counts for a term to be included in the corpus.

**Set at 5**

Use arguments to determine term and doc count mins and maxes, as well as overall vocab max.


```{r}

vocab <- prune_vocabulary(vocab, term_count_min = 5L)

```


Skip Gram Window: Size of context window around the target word

**Set at 5**. Consider 10...


```{r}

# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)

# use window of 5 for context words

# tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

```

## ERROR

> The `tcm` code directly above was not able to run for an unknown reason, even though it ran in a test sample in the (notebook)[analysis.nb.Rmd]. The following code will thus not work to knit until that is resolved.

May need to specify the number of cores, according to author.

```{r}
# glove = GlobalVectors$new(rank = 50, x_max = 10)
# wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
```

```{r}
# wv_context = glove$components
# word_vectors = wv_main + t(wv_context)
```

```{r}
# test <- word_vectors["meeting", , drop = FALSE] -
#   word_vectors["meet", , drop = FALSE] +
#   word_vectors["conference", , drop = FALSE]
# cos_sim = sim2(x = word_vectors, y = test, method = "cosine", norm = "l2")
# head(sort(cos_sim[,1], decreasing = TRUE), 5)


# by <- word_vectors["and", , drop = FALSE] -
#   word_vectors["to", , drop = FALSE] +
#   word_vectors["with", , drop = FALSE]
# cos_sim = sim2(x = word_vectors, y = by, method = "cosine", norm = "l2")
# head(sort(cos_sim[,1], decreasing = TRUE), 5)
```



# Session info

```{r}
sessionInfo()
```
