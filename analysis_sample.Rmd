---
title: "Analysis Sample"
author: "Ben Kubacki"
date: "2024-11-07"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="", fig.path = "images/")

library(tidyverse)
library(readxl)

# web scraping
library(rvest)

# tokenizer
library(tidytext)

# word embeddings
library(text2vec)
library(Rling)
library(cluster)
library(tm)

# visualization
library(tablesgg)
```

# Run pipeline with randomly sampled data

# Run analysis on output csv from full data

# Read in data

> Data is already lemmatized and POS-tagged.

```{r}

news_wlp_sample <- c("data_private/wlp_newspaper_lsp/wlp_news_1990.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1991.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1992.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1993.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1994.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1995.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1996.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1997.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1998.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1999.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2000.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2001.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2002.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2003.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2004.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2005.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2006.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2007.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2008.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2009.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2010.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2011.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2012.txt") %>% 
  map_dfr(~read.delim(.x, header = FALSE, col.names = c("word", "lemma", "pos"), skipNul = TRUE, quote = ""))
  
nrow(news_wlp_sample)
  
news_wlp_sample <- news_wlp_sample %>% as_tibble()

```

# Clean the data

## Tidy columns

- Rename "word", "lemma", "pos"  
- Add document_id column  
- Fill document_id column NAs with document_id

```{r}
## Add column marked by document number (when lemma & pos are NA), keep the rest NA

news_wlp_sample <- news_wlp_sample %>%
  mutate(document_id = ifelse(str_detect(word, "^##\\d{7}")==T, str_sub(word, 3,9), NA)) 

## Note: The following code was not put in the preceding pipe because of knitting issues.

## Replace NAs with document_id 
news_wlp_sample <- news_wlp_sample %>%
  fill(document_id)

news_wlp_sample <- news_wlp_sample %>% 
  slice_sample(n = )

unique(news_wlp$document_id) %>% length()

news_wlp %>% head()

```

## Plan: Write sample data with approx 4,400 words

```{r}

news_sample_2012 <- read_lines("data_private/text_newspaper_lsp/w_news_2012.txt")

news_sample <- c("data_private/text_newspaper_lsp/w_news_2011.txt",
                          "data_private/text_newspaper_lsp/w_news_2012.txt") %>%
  map(read_lines) %>% unlist()

news_sample %>% str_split("\\s+") %>% length()

news_sample_2012 %>% str_split("\\s+") %>% length()

news_sample_2012 <- news_sample_2012 %>% unlist()

news_sample_2012 <- tibble(news_sample_2012)

news_sample_2012 <- news_sample_2012 %>%
  slice(1:10)

news_sample_2012 <- news_sample_2012 %>%
  rename(document_id = news_sample_2012)

news_sample_2012 <- news_sample_2012 %>%
  select(document_id = document_id, text = document_id)

news_sample_2012 <- news_sample_2012 %>% mutate(document_id = str_sub(document_id, 3,9), text = str_sub(text, start = 10))

news_sample_2012 %>%
  map(text[1], str_split("\\s+"))

word_count_row1 <- length(row1_words)

# %>%
  # write_csv("../data/data_sample_text.csv")

```


# Preview

In this Rmd, I have snippets of my code and analysis of the data for a portion of the intended dataset. See Data Sharing at the end of this document.

## Explanation of creation of data sample

Here.   

## Read in data

### w_news_2012.txt

This is the text of each document per row, with the document identifying number in the beginning of the string after two `##`. 


```{r read in data}


text_news_2012 <- read_csv("data/data_sample_text.csv", col_names = F, show_col_types = F) 

head(text_news_2012) %>%
  substring(first = 1, last = 300L) %>% 
  writeLines()

```

### wlp_news_2012.txt

This is the same text but lemmatized and tokenized with one word per row and columns showing word, lemma, and POS.

```{r}
wlp_news_2012 <- read_csv("data/data_sample_tokenized.csv", col_names=F, show_col_types = F) %>% rename(word = X1, lemma = X2, POS = X3)
```


```{r}
wlp_news_2012 %>% 
  summary()

head(wlp_news_2012) %>%
  slice(1:20)

```


## Data in focus: text_news_2012

### Summarize text_news_2012

There are 1559 rows, or documents, and only one column. Below is a sample of the text inside one of the cells, including a break #.

```{r}

summary(text_news_2012)

head(text_news_2012) %>%
  substring(first = 1, last = 300L)

```

Count the words with `str_split()` 

Method from [Sanderson, 2024](https://www.r-bloggers.com/2024/05/counting-words-in-a-string-in-r-a-comprehensive-guide/)

Seems the result is 1331 words, which is safe for sharing. 

```{r}

row1 <- text_news_2012[1, ]

row1_words <- str_split(row1$X1, "\\s+")[[1]]

word_count_row1 <- length(row1_words)

word_count_row1


```


### Separate columns into 2 to isolate the number of the document and the text, then Rename columns:

1559 documents in the original file.

```{r}
text_news_2012 <- text_news_2012 %>% 
  select(document = X1, text = X1) 

text_news_2012$document <- text_news_2012$document %>% 
  str_extract("^.{9}") %>% 
  str_remove("^..") 

text_news_2012$text <- text_news_2012$text %>% 
  str_remove("^.{9}") 

# text_news_2012 %>% 
#   head()                 ## 3706
# text_news_2012 %>% 
#   tail()                 ## 5379
  
text_news_2012$document %>% 
  unique() %>% 
  length()

```

### Format & Description

- \# symbol indicates paragraph breaks  
- punctuation marks have a space on either side  
- @ symbol indicates redacted text? or unreadable text?  

Two methods of counting words, but the second comes from the R-Blogger source above, so may be more reliable than mine.

```{r head and summary}

text_news_2012_row1 <- text_news_2012[1, ]

## Character count, word count per document
text_news_2012_row1 <- text_news_2012_row1 %>% 
  mutate(chr_count = str_count(text_news_2012_row1$text), word_count2 = str_count(text_news_2012_row1$text, "\\s+"))

head(text_news_2012_row1)

```




### Data Sharing

I will include `slice()`d data used in this trial analysis (and later in the full analysis) in the `data/` folder of this repo. The stipulations of the license for the COCA prohibit distributing data of 50,000 or more words, so to be safe, I am only including slices of a sample of rows. This will allow viewers and users to understand the raw data source from which I am drawing, while preserving the protection of the data. The names of the data sources are maintained so that replicators can easily locate and reproduce my results. 

### References 

https://www.corpusdata.org 

Davies, Mark. (2008-) The Corpus of Contemporary American English (COCA). Available online at https://www.english-corpora.org/coca/

# Session Info

```{r}
sessionInfo()
```

