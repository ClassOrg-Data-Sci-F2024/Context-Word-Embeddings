---
title: "Analysis Sample"
author: "Ben Kubacki"
date: "2024-11-07"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="")
library(tidyverse)
library(tidyr)
library(dplyr)
library(purrr)
library(stringr)
library(readxl)

# web scraping
library(rvest)

# tokenizer
library(tidytext)

# word embeddings
library(text2vec)
library(Rling)
library(cluster)
library(tm)

```

# Read in data

- Files: 1 of 22 in (analysis.Rmd)[analysis.Rmd] and (analysis.nb.Rmd)[analysis.nd.Rmd]  
- Type: text  
- Contents: Document number, text   

Method  

- Assign to object `news_sample`  
- Use `c()` to combine all relative paths of the files as strings in character vector  
- Read in using `read_lines()`  
- Use `map()` to iterate `read_lines()` over all 22 files at once  
- Use `unlist()` to turn from list containing character vectors (each doc's text) into character vector containing elements (each doc's text)  
- Read in from `data_private/` folder within this repo but ignored with `.gitignore` 

## Plan: Write sample data with approx 4,400 words

```{r}

# news_sample_2012 <- read_lines("data_private/text_newspaper_lsp/w_news_2012.txt")
# 
# news_sample <- c("data_private/text_newspaper_lsp/w_news_2011.txt",
#                           "data_private/text_newspaper_lsp/w_news_2012.txt") %>% 
#   map(read_lines) %>% unlist()
# 
# news_sample %>% str_split("\\s+") %>% length()
# 
# news_sample_2012 %>% str_split("\\s+") %>% length()
# 
# news_sample_2012 <- news_sample_2012 %>% unlist()
# 
# news_sample_2012 <- tibble(news_sample_2012)
# 
# news_sample_2012 <- news_sample_2012 %>%
#   slice(1:10) 
# 
# news_sample_2012 <- news_sample_2012 %>% 
#   rename(document_id = news_sample_2012)
# 
# news_sample_2012 <- news_sample_2012 %>% 
#   select(document_id = document_id, text = document_id)
# 
# news_sample_2012 <- news_sample_2012 %>% mutate(document_id = str_sub(document_id, 3,9), text = str_sub(text, start = 10)) 
# 
# news_sample_2012 %>% 
#   map(text[1], str_split("\\s+"))

# word_count_row1 <- length(row1_words)

# %>%
#   write_csv("../data/data_sample_text.csv")

```


# Preview

In this Rmd, I have snippets of my code and analysis of the data for a portion of the intended dataset. See Data Sharing at the end of this document.

## Explanation of creation of data sample

Here.   

## Read in data

### w_news_2012.txt

This is the text of each document per row, with the document identifying number in the beginning of the string after two `##`. 


```{r read in data}
text_news_2012 <- read_csv("data/data_sample_text.csv", col_names = F, show_col_types = F) 

head(text_news_2012) %>%
  substring(first = 1, last = 300L) %>% 
  writeLines()

```

### wlp_news_2012.txt

This is the same text but lemmatized and tokenized with one word per row and columns showing word, lemma, and POS.

```{r}
wlp_news_2012 <- read_csv("data/data_sample_tokenized.csv", col_names=F, show_col_types = F) %>% rename(word = X1, lemma = X2, POS = X3)
```


```{r}
wlp_news_2012 %>% 
  summary()

head(wlp_news_2012) %>%
  slice(1:20)

```


## Data in focus: text_news_2012

### Summarize text_news_2012

There are 1559 rows, or documents, and only one column. Below is a sample of the text inside one of the cells, including a break #.

```{r}

summary(text_news_2012)

head(text_news_2012) %>%
  substring(first = 1, last = 300L)

```

Count the words with `str_split()` 

Method from [Sanderson, 2024](https://www.r-bloggers.com/2024/05/counting-words-in-a-string-in-r-a-comprehensive-guide/)

Seems the result is 1331 words, which is safe for sharing. 

```{r}

row1 <- text_news_2012[1, ]

row1_words <- str_split(row1$X1, "\\s+")[[1]]

word_count_row1 <- length(row1_words)

word_count_row1


```


### Separate columns into 2 to isolate the number of the document and the text, then Rename columns:

1559 documents in the original file.

```{r}
text_news_2012 <- text_news_2012 %>% 
  select(document = X1, text = X1) 

text_news_2012$document <- text_news_2012$document %>% 
  str_extract("^.{9}") %>% 
  str_remove("^..") 

text_news_2012$text <- text_news_2012$text %>% 
  str_remove("^.{9}") 

# text_news_2012 %>% 
#   head()                 ## 3706
# text_news_2012 %>% 
#   tail()                 ## 5379
  
text_news_2012$document %>% 
  unique() %>% 
  length()

```

### Format & Description

- \# symbol indicates paragraph breaks  
- punctuation marks have a space on either side  
- @ symbol indicates redacted text? or unreadable text?  

Two methods of counting words, but the second comes from the R-Blogger source above, so may be more reliable than mine.

```{r head and summary}

text_news_2012_row1 <- text_news_2012[1, ]

## Character count, word count per document
text_news_2012_row1 <- text_news_2012_row1 %>% 
  mutate(chr_count = str_count(text_news_2012_row1$text), word_count2 = str_count(text_news_2012_row1$text, "\\s+"))

head(text_news_2012_row1)

```




### Data Sharing

I will include `slice()`d data used in this trial analysis (and later in the full analysis) in the `data/` folder of this repo. The stipulations of the license for the COCA prohibit distributing data of 50,000 or more words, so to be safe, I am only including slices of a sample of rows. This will allow viewers and users to understand the raw data source from which I am drawing, while preserving the protection of the data. The names of the data sources are maintained so that replicators can easily locate and reproduce my results. 

### References 

https://www.corpusdata.org 

Davies, Mark. (2008-) The Corpus of Contemporary American English (COCA). Available online at https://www.english-corpora.org/coca/

# Session Info

```{r}
sessionInfo()
```

