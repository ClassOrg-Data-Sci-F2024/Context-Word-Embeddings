---
title: "Analysis of Workplace-Themed Term Co-Occurrence Matrices"
author: "Ben Kubacki"
date: "13 December 2024"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="", fig.path = "images/")

library(tidyverse)
library(readxl)

# web scraping
library(rvest)

# tokenizer
library(tidytext)

# word embeddings
library(text2vec)
library(Rling)
library(cluster)
library(tm)
```



# Read in data

> Data is already lemmatized and POS-tagged.

```{r}

# c("data_private/wlp_newspaper_lsp/wlp_news_2012.txt") %>% 
#   read_tsv(col_names = F)
  
  news_wlp <- c("data_private/wlp_newspaper_lsp/wlp_news_1990.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1991.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1992.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1993.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1994.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1995.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1996.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1997.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1998.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1999.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2000.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2001.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2002.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2003.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2004.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2005.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2006.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2007.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2008.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2009.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2010.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2011.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2012.txt") %>% 
  read_tsv(col_names = F)

```
# Clean the data

## Tidy columns

- Rename "word", "lemma", "pos"  
- Add document_id column  
- Fill document_id column NAs with document_id

```{r}

## Rename columns
news_wlp <- news_wlp %>%
  select(word = X1, lemma = X2, pos = X3)

## Add column marked by document number (when lemma & pos are NA), keep the rest NA
news_wlp <- news_wlp %>%
  mutate(document_id = ifelse(is.na(news_wlp$pos) & is.na(news_wlp$lemma), str_sub(word, 3,9),NA))

## Replace NAs with document_id 
news_wlp <- news_wlp %>%
  fill(document_id)

news_wlp %>% head()

```

## Filter POS to include only adjectives, nouns, and verbs. 

> Compare full list against included list in code chunk

POS Tags [reference list](https://ucrel.lancs.ac.uk/claws7tags.html)

- Some data is marked with multiple POS (i.e., jj_nn)  
- Create regex separated by `|` to filter for all cases, including multiples

```{r}

pos_filtered <- c("jj", "jjr", "jjt", "jk", "nn", "nn1", "nn2", "nna", "nnb", "nnl1", "nnl2", "nno", "nno2", "nnt1", "nnt2", "nnu", "nnu1", "nnu2", "vv0", "vvd", "vvg", "vvgk", "vvi", "vvn", "vvnk", "vvz")

pos_filtered_c <- str_c(pos_filtered, collapse="|")

news_wlp <- news_wlp %>% 
  filter(str_detect(pos, pos_filtered_c))

news_wlp %>% head()

```

# Build a TCM Matrix

Source of demo, including object names, code, and comments inside code: (Selivanov, 2023)[https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html]

## Tokens

In this step:  

- Remove punctuation and contractions (may already be removed from POS filtering)  
- Normalize for capitalization by converting to lower case

```{r}

## Create tokens object as list

tokens <- news_wlp %>% select(document_id, lemma) %>% chop(lemma) %>% pull(lemma, document_id) #%>% 
  #as.list()

stop_punctuation_regex <- "[<p>]|[\\,\\.\\:\\;\\*\\@\\'\\\"\\-\\$\\(\\)\\?]|(\\-\\-)|('s)|('d)|('t)|(n't)|('ve)|('re)|('ll)"

tokens <- map(tokens, ~ str_to_lower(str_subset(.x, stop_punctuation_regex, negate=T)))

## Output summary
str(tokens)
summary(tokens)

## Output shows first element of list
tokens[[1]] %>% head(50)

```

## Create iterator

```{r}

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)

```

### Create and filter vocabulary for infrequent terms and stop-words

#### Create stop-word list

```{r}

## Make table with tidytext "snowball" and tm "en" and my own additions
  
# tidytext
stop_words_snowball <- filter(stop_words, lexicon=="snowball") %>% select(word)

# tm::stopwords(kind = "en")

# My additions
my_stopwords <- c("also", "even", "one", "yet", "y", "mr", "ms")

# Aggregated
stop_word_list <- c(stopwords(kind="en"), stop_words_snowball$word, my_stopwords)

stop_word_list <- unique(stop_word_list)

stop_word_list

```

### Create vocabulary

**Add stopwords in argument in create_vocabulary function**??

```{r}
vocab <- create_vocabulary(it, stopwords = stop_word_list)

# Remove very infrequent terms using min term count =2
```


**Determine if need to remove frequent terms**

- Don't remove because some relevant content words

```{r}

# vocab <- vocab %>% filter(!term %in% stop_word_list)

## Determine which max count you want to remove if any
# vocab %>% arrange(desc(term_count)) %>% head(100)

# add argument term_count_max = 170000L)
vocab <- prune_vocabulary(vocab, term_count_min = 2L)


```

### Create vectorizer and term co-occurrence matrix

```{r}
vectorizer <- vocab_vectorizer(vocab)

tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

```

### Create vectors

```{r}

glove <-  GlobalVectors$new(rank = 50, x_max = 10)

wv_main <-  glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)

wv_context = glove$components

word_vectors = wv_main + t(wv_context)

```


# Create Target Word List

## Web-scraped

**Promova**

```{r}

promova_list <- read_html("https://promova.com/english-vocabulary/occupations-and-jobs-english-vocabulary") %>% 
  html_elements("li span") %>% 
  html_text()

promova_list <- promova_list[10:63] %>% 
  str_extract("^.+\\:") %>% 
  str_to_lower %>% 
  str_remove("\\:")

promova_list <- promova_list[27:54] 

promova_table <- promova_list %>% 
  tibble(word = `.`) 

stoplist <- c("cover letter", "job description", "background check", "probationary period", "job offer", "telecommuting/remote working", "salary expectations", "out of office")

promova_table <- promova_table %>% 
  filter(!word %in% stoplist)

promova_table$word <- promova_table$word %>% 
  str_remove("ing")

```


**Gorick**


```{r}

gorick_list <- read_html("https://www.gorick.com/blog/workplace-jargon-dictionary")

gorick_list <- gorick_list %>%
  html_elements("h3") %>%
  html_text()

gorick_list <- gorick_list %>%
  str_remove("^.") %>%
  str_remove(".$")

gorick_list <- gorick_list %>% 
  str_subset("^[A-Z]{2,9}$", negate=T)

gorick_list <- gorick_list %>%
  tolower()

gorick_list

  
```


**My own ideas with insight from gorick list**

c("work", "meeting", "job", "interview", "evaluate", "orientation", "communicate", "career", "task", "action", "value", "benchmark", "practice", "business", "consult", "context", "deliver") 


## Cosine similarities for Term Co-Occurrence Matrix

Cosine similarities per target word: 

"work" should include both nouns and verbs, unfortunately

```{r}

# tokens <- map(tokens, ~ str_to_lower(str_subset(.x, stop_punctuation_regex, negate=T)))

map(~ sim2(x=word_vectors, y = word_vectors[.x, , drop = FALSE], method = "cosine", norm = "l2") .x )

```



```{r}

## Target word "work"
cos_work <- sim2(x = word_vectors, y = word_vectors["work", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

## Target word "meeting"
cos_meeting <- sim2(x = word_vectors, y = word_vectors["meeting", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

## Target word "job"
cos_job <- sim2(x = word_vectors, y = word_vectors["job", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

## Target word "interview"
cos_interview <- sim2(x = word_vectors, y = word_vectors["interview", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

## Target word "review"
cos_evaluate <- sim2(x = word_vectors, y = word_vectors["evaluate", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

## Target word "orientation"
cos_orientation <- sim2(x = word_vectors, y = word_vectors["orientation", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

## Target word "onboard"
cos_onboard <- sim2(x = word_vectors, y = word_vectors["onboard", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)


```



```{r}
cos_tbl_work <- as_tibble(cos_work, rownames="words") %>% 
  rename(cos_sim = value) %>% 
  mutate(target = "work") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5)

cos_tbl_meeting <- as_tibble(cos_meeting, rownames="words") %>% 
  rename(cos_sim = value) %>% 
  mutate(target = "meeting") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5)

cos_tbl_job <- as_tibble(cos_job, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "job") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5)

cos_tbl_interview <- as_tibble(cos_interview, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "interview") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5)

cos_tbl_evaluate <- as_tibble(cos_evaluate, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "evaluate") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "evaluate"))

cos_tbl_orientation <- as_tibble(cos_orientation, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "orientation") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "orientation"))

cos_tbl_onboard <- as_tibble(cos_onboard, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "onboard") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "onboard"))

## Add target word to facilitate join


```

Combine table names into C()

```{r}

cos_tbl_c <- c("cos_tbl_work", 
               "cos_tbl_meeting",
               "cos_tbl_job",
               "cos_tbl_interview",
               "cos_tbl_evaluate") %>% writeLines()

```


```{r}


## Join the target word tibbles
cos_tbl1 <- full_join(cos_tbl_work, cos_tbl_meeting)

cos_tbl2 <- full_join(cos_tbl_job, cos_tbl_interview)

cos_tbl3 <- full_join(cos_tbl1, cos_tbl2)

cos_tbl <- full_join(cos_tbl3, cos_tbl_evaluate)

cos_tbl %>% arrange(desc(cos_sim))

cos_tbl <- bind_rows(cos_tbl_work,
                     cos_tbl_meeting,
                     cos_tbl_job,
                     cos_tbl_interview,
                     cos_tbl_evaluate,
                     cos_tbl_orientation,
                     cos_tbl_onboard)

```

Write CSV

```{r}

# cos_tbl %>% slice_sample() %>% write_csv()

```

Visualize

```{r}

cos_tbl %>% 
  filter(target == "work") %>% 
  arrange(cos_sim)

```


Plot along line

First we need to create a new table with the target word cosine similarities in one column and the other target words in another column with their cosines in another column. 

```{r plotcompare, dependson = "cos_tbl", fig.width=10, fig.height=5, fig.cap="Comparing the Cosine Similarities of Target Words"}

cos_tbl_work2 <-
  cos_tbl %>% 
  pivot_wider(names_from = target, values_from = cos_sim) %>%
  pivot_longer(meeting:evaluate,
               names_to = "target", values_to = "cos_sim")

ggplot(cos_tbl_work2, aes(x = cos_sim, y = work,
                          color = abs(work - cos_sim))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(limits = c(0, 0.5), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~target, ncol = 3) +
  theme(legend.position="none") +
  labs(y = "work", x = NULL)


```

Plot by target word pairs

```{r plotcompare, dependson = "cos_tbl", fig.width=10, fig.height=5, fig.cap="Comparing the Cosine Similarities of Pairs of Target Words"}

unique(cos_tbl$target) %>% writeLines()

cos_tbl_orientation_onboard <-
  cos_tbl %>% 
  pivot_wider(names_from = target, values_from = cos_sim) %>%
  select(words, orientation, onboard) %>% 
  pivot_longer(onboard, names_to = "target", values_to = "cos_sim")

ggplot(cos_tbl_orientation_onboard, aes(x = cos_sim, y = orientation,
                          color = abs(orientation - cos_sim))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(limits = c(0, 0.5), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~target, ncol = 3) +
  theme(legend.position="none") +
  labs(y = "orientation", x = NULL)


```
This didn't work, which might suggest that there is not any overlapping words between these two. 

Examine each individually and see . But they should still show up...


```{r}
cos_tbl %>% arrange(cos_sim)
# 
# ggplot(slice(cos_tbl), aes(x = words, y = cos_sim)) + geom_col()

```


```{r}

# read_csv("data_private/test_cos_25k.csv")

# data_work <- read_csv("data_private/cosine_work.csv")
# 
# data_work %>% head(20)
# 
# data_work %>% summary()
# 
# data_work %>% 
#   head(21) %>% 
#   ggplot(aes(words, cos_sim))+
#   geom_point(alpha = 0)+
#   geom_text(aes(label = words))


cos_tbl %>% 
  group_by(target) %>% 
  slice(1:21) %>% 
  ggplot(aes(words, cos_sim))+
  geom_point(alpha = 0)+
  geom_text(aes(label = words, colour = target))
  
```


```{r fig.align="center", echo = FALSE,fig.width = 14}
cos_tbl %>% 
  group_by(target) %>% 
  slice(1:20) %>% 
  ggplot(aes(words, cos_sim))+
  geom_point(alpha = 0)+
  geom_text(aes(label = words, colour = target))

# data_work %>% 
#   head(50) %>% 
#   ggplot(aes(words, cos_sim))+
#   geom_point(alpha = 0)+
#   geom_text(aes(label = words))
```

# Session info

```{r}
sessionInfo()
```
