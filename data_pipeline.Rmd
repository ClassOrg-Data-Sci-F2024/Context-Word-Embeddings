---
title: "Analysis of Workplace-Themed Term Co-Occurrence Matrices"
author: "Ben Kubacki"
date: "13 December 2024"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="", fig.path = "images/")

library(tidyverse)
library(readxl)

# web scraping
library(rvest)

# tokenizer
library(tidytext)

# word embeddings
library(text2vec)
library(Rling)
library(cluster)
library(tm)

# visualization
library(tablesgg)
```



# Read in data

> Data is already lemmatized and POS-tagged.

```{r}

news_wlp <- c("data_private/wlp_newspaper_lsp/wlp_news_1990.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1991.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1992.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1993.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1994.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1995.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1996.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1997.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1998.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_1999.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2000.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2001.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2002.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2003.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2004.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2005.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2006.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2007.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2008.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2009.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2010.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2011.txt",
           "data_private/wlp_newspaper_lsp/wlp_news_2012.txt") %>% 
  map_dfr(~read.delim(.x, header = FALSE, col.names = c("word", "lemma", "pos"), skipNul = TRUE, quote = ""))
  
nrow(news_wlp)
  
news_wlp <- news_wlp %>% as_tibble()

```


# Clean the data

## Tidy columns

- Rename "word", "lemma", "pos"  
- Add document_id column  
- Fill document_id column NAs with document_id

```{r}
## Add column marked by document number (when lemma & pos are NA), keep the rest NA

news_wlp <- news_wlp %>%
  mutate(document_id = ifelse(str_detect(word, "^##\\d{7}")==T, str_sub(word, 3,9), NA)) 

## Note: The following code was not put in the preceding pipe because of knitting issues.

## Replace NAs with document_id 
news_wlp <- news_wlp %>%
  fill(document_id)

unique(news_wlp$document_id) %>% length()

news_wlp %>% head()

```

## Filter POS to include only adjectives, nouns, and verbs. 

> Compare full list against included list in code chunk

POS Tags [reference list](https://ucrel.lancs.ac.uk/claws7tags.html)

- Some data is marked with multiple POS (i.e., jj_nn)  
- Create regex separated by `|` to filter for all cases, including multiples  
- Punctuation is marked as "y" in COCA tags, so this step (not including "y") should be sufficient to remove most punctuation, though not contractions

```{r}

pos_keep <- c("jj|jk|nn|vv")

news_wlp <- news_wlp %>% 
  filter(str_detect(pos, pos_keep))

news_wlp %>% head()

news_wlp %>% nrow()

unique(news_wlp$document_id) %>% length()

```
Filtering the dataset to keep only adjectives, nouns, and verbs reduced it to 37k rows from 108mil rows (more accurately, "words" now because punctuation is removed by pos == "y"), but the number of documents is still 57026. 


# Build a TCM Matrix

Source of demo, including object names, code, and comments inside code: (Selivanov, 2023)[https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html]

## Tokens

In this step:  

- Remove punctuation and contractions (may already be removed from POS filtering)  
- Normalize for capitalization by converting to lower case

```{r}

## Create punctuation and contractions list to be removed just in case
stop_punctuation_regex <- "[\\,\\.\\:\\;\\*\\@\\'\\\"\\-\\$\\(\\)\\?]|(\\-\\-)|('s)|('d)|('t)|(n't)|('ve)|('re)|('ll)"

## Create tokens object as list

tokens <- news_wlp %>% select(document_id, lemma) %>% chop(lemma) %>% pull(lemma, document_id) #%>% 
  #as.list()

tokens <- map(tokens, ~ str_to_lower(str_subset(.x, stop_punctuation_regex, negate=T)))

## Output summary
summary(tokens) %>% head()

## Output shows first element of list
tokens[[1]] %>% head(50)

```

## TCM Continued

- Iterator  
- Vocabulary & Remove Stopwords   
- Vectorizer  
- Term co-occurrence matrix  
- Word vectors

```{r}

# Create iterators. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)

##Create and filter vocabulary for infrequent terms and stop-words

## Create stopwords

## Make table with tidytext "snowball" and tm "en" and my own additions
  
# tidytext
stop_words_snowball <- filter(stop_words, lexicon=="snowball") %>% select(word)

# tm::stopwords(kind = "en")

# My additions
my_stopwords <- c("also", "even", "one", "yet", "mr", "ms")

# Aggregated
stop_word_list <- c(stopwords(kind="en"), stop_words_snowball$word, my_stopwords)

stop_word_list <- unique(stop_word_list)

stop_word_list

```

### Create vocabulary and vectorizer

**Add stopwords in argument in create_vocabulary function**??

**Determine if need to remove frequent terms**

- Don't remove because some relevant content words

```{r}
vocab <- create_vocabulary(it, stopwords = stop_word_list)


# vocab <- vocab %>% filter(!term %in% stop_word_list)

## Determine which max count you want to remove if any
# vocab %>% arrange(desc(term_count)) %>% head(100)

# Remove very infrequent terms using min term count =2
# add argument term_count_max = 170000L)

vocab <- prune_vocabulary(vocab, term_count_min = 2L)

vectorizer <- vocab_vectorizer(vocab)

```

### Create Term Co-occurrence Matrix and Vectors

```{r}

tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)


glove <-  GlobalVectors$new(rank = 50, x_max = 10)

wv_main <-  glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)

wv_context = glove$components

word_vectors = wv_main + t(wv_context)

```


# Seed List of Target Words

1. Web-scraped lists

**Promova**

```{r}

promova_list <- read_html("https://promova.com/english-vocabulary/occupations-and-jobs-english-vocabulary") %>% 
  html_elements("li span") %>% 
  html_text()

promova_list <- promova_list[10:63] %>% 
  str_extract("^.+\\:") %>% 
  str_to_lower %>% 
  str_remove("\\:")

promova_list <- promova_list[27:54] 

promova_table <- promova_list %>% 
  tibble(word = `.`) 

stoplist <- c("cover letter", "job description", "background check", "probationary period", "job offer", "telecommuting/remote working", "salary expectations", "out of office")

promova_table <- promova_table %>% 
  filter(!word %in% stoplist)

promova_table$word <- promova_table$word %>% 
  str_remove("ing")

```


**Gorick**


```{r}

gorick_list <- read_html("https://www.gorick.com/blog/workplace-jargon-dictionary")

gorick_list <- gorick_list %>%
  html_elements("h3") %>%
  html_text()

gorick_list <- gorick_list %>%
  str_remove("^.") %>%
  str_remove(".$")

gorick_list <- gorick_list %>% 
  str_subset("^[A-Z]{2,9}$", negate=T)

gorick_list <- gorick_list %>%
  tolower()

  
```


### My own curated list 


**My own ideas with insight from gorick list**

- Used `writeLines()` in console to paste the seed_list_1 with 1 appended into the next code chunk, and same with seed_list_2

```{r}

seed_list <- c("meeting", "job", "interview", "apply", "communicate", "career", "task", "action", "value", "benchmark", "practice", "business", "consult") 

seed_list1 <- paste0(seed_list, "1")

seed_list2 <- paste0(seed_list, "2")

seed_list_sorted <- sort(c("meeting", "job", "interview", "apply", "communicate", "career", "task", "action", "value", "benchmark", "practice", "business", "consult"), decreasing=F) 
```


# Cosine similarities based on Term Co-Occurrence Matrix

Target word cosine similarities: target1

```{r target1 sim2 objects}

# map(seed_list, ~ sort(sim2(x = word_vectors, y = word_vectors[.x, , drop = FALSE], method = "cosine", norm = "l2"), decreasing = T)) %>% 
#   map(cos_seed_list, ~ as_tibble(.x, rownames="words")) %>% 
#   unlist()

meeting1 <- sim2(x = word_vectors, y = word_vectors["meeting", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

job1 <- sim2(x = word_vectors, y = word_vectors["job", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

interview1 <- sim2(x = word_vectors, y = word_vectors["interview", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

apply1 <- sim2(x = word_vectors, y = word_vectors["apply", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

communicate1 <- sim2(x = word_vectors, y = word_vectors["communicate", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

career1 <- sim2(x = word_vectors, y = word_vectors["career", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

task1 <- sim2(x = word_vectors, y = word_vectors["task", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

action1 <- sim2(x = word_vectors, y = word_vectors["action", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

value1 <- sim2(x = word_vectors, y = word_vectors["value", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

benchmark1 <- sim2(x = word_vectors, y = word_vectors["benchmark", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

practice1 <- sim2(x = word_vectors, y = word_vectors["practice", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

business1 <- sim2(x = word_vectors, y = word_vectors["business", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

consult1 <- sim2(x = word_vectors, y = word_vectors["consult", , drop=FALSE], method = "cosine", norm = "l2")[,0:1] %>% sort(decreasing = T)

```


Target word tables: target2

```{r target2 table objects}

meeting2 <- as_tibble(meeting1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "meeting") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "meeting"))

job2 <- as_tibble(job1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "job") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "job"))

interview2 <- as_tibble(interview1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "interview") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "interview"))

apply2 <- as_tibble(apply1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "apply") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "apply"))

communicate2 <- as_tibble(communicate1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "communicate") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "communicate"))

career2 <- as_tibble(career1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "career") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "career"))

task2 <- as_tibble(task1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "task") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "task"))

action2 <- as_tibble(action1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "action") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "action"))

value2 <- as_tibble(value1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "value") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "value"))

benchmark2 <- as_tibble(benchmark1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "benchmark") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "benchmark"))

practice2 <- as_tibble(practice1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "practice") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "practice"))

business2 <- as_tibble(business1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "business") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "business"))

consult2 <- as_tibble(consult1, rownames="words") %>% 
  rename(cos_sim = value) %>%
  mutate(target = "consult") %>% 
  filter(cos_sim < 1, cos_sim >= 0.5) %>% 
  filter(!str_detect(words, "consult"))


```

Join the target word tibbles

```{r}

## Join the target word tibbles

cos_tbl <- bind_rows(meeting2,
               job2, 
               interview2, 
               apply2,
               communicate2,
               career2,
               task2,
               action2,
               value2,
               benchmark2,
               practice2,
               business2,
               consult2)

```

Analyze top 15 words for each seed word

**Use this to present the data in the report**

Create top 15 output words per target word, longform

```{r create top 15 output words per target word}

cos_tbl_15 <- cos_tbl %>% 
  group_by(target) %>% 
  slice(1:15)
```


```{r plotcompare, dependson = "cos_tbl_15", fig.width=10, fig.height=5, fig.show = "hold", fig.cap="15 Most Similar Words by Target Word", dev = "png"}


cos_tbl_15_words <- cos_tbl_15 %>% 
  group_by(target) %>% 
  arrange(words) %>% 
  summarize(words = str_flatten(words, ", "))

cos_df_15_words <- as.data.frame(cos_tbl_15_words)

plot.textTable(textTable(cos_df_15_words, title = "Top 15 Output Words", subtitle = "Cosine Similarities in Descending Order by Target Word", row.names=F))

```

Write CSV

```{r}

cos_tbl %>% 
  group_by(target) %>% 
  slice(1:100) %>% 
  write_csv("data/cosine_similarities.csv")

```

# Visualizations

## Plot by target word pairs

First we need to create a new table with the target word cosine similarities in one column and the other target words in another column with their cosines in another column. 

**Target word Interview**

```{r plotcompare2, dependson = "cos_tbl", fig.show = "hold", fig.width=10, fig.height=15, fig.cap="Comparing the Cosine Similarities of Pairs of Target Words"}

cos_tbl_interview <-
  cos_tbl %>% 
  pivot_wider(names_from = target, values_from = cos_sim) %>% 
  pivot_longer(cols = c(seed_list, -interview), names_to = "target", values_to = "cos_sim")

ggplot(cos_tbl_interview, aes(x = cos_sim, y = interview,
                          color = abs(interview - cos_sim))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(low = "darkblue", high = "red") +
  facet_wrap(~target, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "\"Interview\" Cosine Similarities", x = "Cosine Similarities by Target Word")


```

### Check "question" output word against "interview" and "action". Should be around 0.75 for each.

```{r}

cos_tbl %>% 
  filter(words == "question") %>% 
  arrange(desc(cos_sim))

```

Indeed, "question" is between 0.77 and 0.78 similar to both "interview" and "action".


```{r plotcompare3, dependson = "cos_tbl", fig.show = "hold", fig.width=10, fig.height=5, fig.cap="Comparing Interview & Apply Similar Words"}

ggplot(filter(cos_tbl_interview, target == "apply"), aes(x = cos_sim, y = interview,
                          color = abs(interview - cos_sim))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5) +
  scale_color_gradient(low = "darkblue", high = "red") +
  theme(legend.position="none") +
  labs(y = "\"Interview\" Cosine Similarities", x = "\"Apply\" Cosine Similarities")


```

## Overlap Between Output Words

**Determine if there are output words that overlap between target words when cosine > 0.7. For example, are there words in the range of >0.7 cos_sim that are the same for both "task" and "action"?**

- if we pivot wider, we can inspect the table to find words that have cos_sim values rather than NA  
- if we first filter out cos_sim < 7, it will show us what we are working with  
- 

```{r}

cos_wide <- cos_tbl %>% 
  filter(cos_sim>=.6) %>% 
  pivot_wider(names_from = target, values_from = cos_sim) 

cos_wide <- cos_wide %>% 
  mutate(na_count = rowSums(is.na(.))) %>% 
  filter(na_count < 12) %>% 
  arrange(na_count)

cos_wide


```

The words that have the most overlap among the target words are (with only five target words not matching): 

- change  
- give  
- reason  
- important  


```{r plotcompare4, fig.align="center", echo = FALSE,fig.width = 14, fig.show = "hold"}

cos_tbl %>% 
  group_by(target) %>% 
  slice(1:10) %>% 
  ggplot(aes(words, cos_sim))+
  geom_point(alpha = 0)+
  geom_text(aes(label = words, colour = target), check_overlap = TRUE)+
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())+
  labs(y = "Cosine Similarity with Target Word", x = "Output Words")

```

# Session info

```{r}
sessionInfo()
```
