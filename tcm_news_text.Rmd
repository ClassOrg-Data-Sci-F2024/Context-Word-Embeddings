---
title: "Term Co-Occurrence Matrix Newspaper Text Files"
author: "Ben Kubacki"
date: "2024-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="")
library(tidyverse)
library(text2vec)
library(tidytext)
library(tm)
```

Source of demo: (Selivanov, 2023)[https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html]

"You can achieve much better results by experimenting with skip_grams_window and the parameters of the GloVe class (including word vectors size and the number of iterations)."

## Now COCA

```{r}

vector_sample <- read_csv("data/data_sample_text.csv", col_names=F)

vector_sample <- vector_sample %>% 
  select(document = X1, text = X1) 

vector_sample$document <- vector_sample$document %>% 
  str_extract("^.{9}") %>% 
  str_remove("^..") 

vector_sample$text <- vector_sample$text %>% 
  str_remove("^.{9}") 

## Remove stop words

# vector_sample$text <- vector_sample$text %>% 
#   str_subset(stop_words$word, negate = T, lexicon = "snowball")

```

use glove

## Let's try the tcm version:

```{r}
# Create iterator over tokens
tokens <- space_tokenizer(vector_sample$text)

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
```

Use source's vocab list first, change later to mine.

```{r}

# ?prune_vocabulary

vocab <- prune_vocabulary(vocab, term_count_min = 5L)

```

```{r}

# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)

# use window of 5 for context words
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

```

Specify the number of cores?

```{r}
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
```

```{r}
wv_context = glove$components
word_vectors = wv_main + t(wv_context)
```

```{r}
# test <- word_vectors["meeting", , drop = FALSE] - 
#   word_vectors["meet", , drop = FALSE] + 
#   word_vectors["conference", , drop = FALSE]
# cos_sim = sim2(x = word_vectors, y = test, method = "cosine", norm = "l2")
# head(sort(cos_sim[,1], decreasing = TRUE), 5)


by <- word_vectors["and", , drop = FALSE] - 
  word_vectors["to", , drop = FALSE] + 
  word_vectors["with", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = by, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

I think it worked!

```{r}

tbl <- tibble(word = c(cos_sim[,0]), cos_sim = cos_sim[,1])
cos_sim[,1]
  
cos_sim[,1] %>%
  sort(decreasing = T) %>% 
  str_extract("[^\\d]")

```

### For instructions on Create DTM: 

- Silge & Robinson 2024 [here](https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html)

- Selivanov (2023) [here](https://cran.r-project.org/web/packages/text2vec/text2vec.pdf)

iterate over tokens

```{r}

# it2 <- itoken(vector_sample$text, preprocess_function=tolower, tokenizer=word_tokenizer)
# 
# v2 = create_vocabulary(it2)
# 
# pruned_vocab2 <- prune_vocabulary(v2, term_count_min = 10,
#                                  doc_proportion_max = 0.5,
#                                  doc_proportion_min = 0.001)
# vectorizer2 <- vocab_vectorizer(v2)
# 
# it2 <- itoken(vector_sample$text, preprocess_function=tolower, tokenizer=word_tokenizer)
# 
# dtm <- create_dtm(it2, vectorizer2, type = "CsparseMatrix")

# tokens <- as.vector(unnest_tokens(vector_sample, word, text, token = "words", to_lower=T)[,2] )
# 
# it <- itoken(tokens2, progressbar = F)
# 
# dtm <- create_dtm(it2, vectorizer)
```
# Session info

```{r}
sessionInfo()
```
