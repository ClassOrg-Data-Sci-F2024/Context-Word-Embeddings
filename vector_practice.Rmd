---
title: "vector_practice"
author: "Ben Kubacki"
date: "2024-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment="")
library(tidyverse)
library(text2vec)
library(tidytext)
library(tm)
```

# 1. Selivanov's Text2Vec Demo 

Source: (Selivanov, 2023)[https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html]

Get the data (not sure the format):

```{r}
text8_file <-  "~/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "~/text8.zip")
  unzip ("~/text8.zip", files = "text8", exdir = "~/")
}
wiki <-  readLines(text8_file, n = 1, warn = FALSE)
```

Create a vocabulary: A set of words for which we want to learn word vectors.

Need to iterate over tokens as the first argument for this `create_vocabulary()` function and similar ones like `create_dtm()`

```{r}
# Create iterator over tokens
tokens <- space_tokenizer(wiki)

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
```

Use source's vocab list first, change later to mine.

```{r}

# ?prune_vocabulary

vocab <- prune_vocabulary(vocab, term_count_min = 5L)

```

```{r}

# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)

# use window of 5 for context words
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

```

Specify the number of cores?

```{r}
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
```

```{r}
wv_context = glove$components
word_vectors = wv_main + t(wv_context)
```

```{r}
berlin <- word_vectors["paris", , drop = FALSE] - 
  word_vectors["france", , drop = FALSE] + 
  word_vectors["germany", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = berlin, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

"You can achieve much better results by experimenting with skip_grams_window and the parameters of the GloVe class (including word vectors size and the number of iterations)."

```{r}

tbl <- tibble(word = c(cos_sim[,0]), cos_sim = cos_sim[,1])
cos_sim[,1]
  
cos_sim[,1] %>%
  sort(decreasing = T) %>% 
  str_extract("[^\\d]")

```


## Now COCA

```{r}

vector_sample <- read_csv("../data/data_sample_text.csv", col_names=F)

vector_sample <- vector_sample %>% 
  select(document = X1, text = X1) 

vector_sample$document <- vector_sample$document %>% 
  str_extract("^.{9}") %>% 
  str_remove("^..") 

vector_sample$text <- vector_sample$text %>% 
  str_remove("^.{9}") 

```

iterate over tokens

```{r}

it2 <- itoken(vector_sample$text, preprocess_function=tolower, tokenizer=word_tokenizer)

v2 = create_vocabulary(it2)

pruned_vocab2 <- prune_vocabulary(v2, term_count_min = 10,
                                 doc_proportion_max = 0.5,
                                 doc_proportion_min = 0.001)
vectorizer2 <- vocab_vectorizer(v2)

it2 <- itoken(vector_sample$text, preprocess_function=tolower, tokenizer=word_tokenizer)

dtm <- create_dtm(it2, vectorizer2, type = "CsparseMatrix")

# tokens <- as.vector(unnest_tokens(vector_sample, word, text, token = "words", to_lower=T)[,2] )
# 
# it <- itoken(tokens2, progressbar = F)
# 
# dtm <- create_dtm(it2, vectorizer)
```

use glove

```{r}
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(dtm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
```

## Let's try the tcm version:

```{r}
# Create iterator over tokens
tokens <- space_tokenizer(vector_sample$text)

# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
```

Use source's vocab list first, change later to mine.

```{r}

# ?prune_vocabulary

vocab <- prune_vocabulary(vocab, term_count_min = 5L)

```

```{r}

# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)

# use window of 5 for context words
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

```

Specify the number of cores?

```{r}
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
```

```{r}
wv_context = glove$components
word_vectors = wv_main + t(wv_context)
```

```{r}
test1 <- word_vectors["meeting", , drop = FALSE] - 
  word_vectors["meet", , drop = FALSE] + 
  word_vectors["conference", , drop = FALSE]

cos_sim = sim2(x = word_vectors, y = test1, method = "cosine", norm = "l2")

head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

I think it worked!









# 2. Levshina's p. 323

> Use :: to specify `text2vec` versus `Rling` packages for function use

```{r}

# install.packages("cluster")
library("Rling")
library("cluster")
data(cooking)
head(cooking)

```

To make the matrices above, "For a bag-of-words vector space model, collocates (lemmas) were selected within the window of five words on the left and five words on the right. Next, a matrix of co-occurrences was created, which can be found in the data frame `cooking`" (p. 326).

Next, 

0. Create either a term-term (bag-of-words) or document-term matrix.  
- separate the documents in my data  
- use `create_dtm` to make the dtm matrix  

1. Create semantic vectors from weighted co-occurrence frequencies  
- remove rows containing only zeros  
- compute expected co-occurrence frequencies  
- compute PMI scores  
- transform the latter into PPMI scores  
2. Compute similarity scores between the resulting PPMI vectors with the help of the cosine measure  
3. Explore similarity scores (with cluster analysis as needed)

Outcome

```{r}

cooking <- cooking[rowSums(cooking) > 0, ]

exp.bake <- sum(cooking$Bake)*rowSums(cooking)/sum(cooking)
exp.bake[1:12]

pmi.bake <- log2(cooking$Bake/exp.bake)
pmi.bake[1:12]

## PPMI negative values replaced with zero

ppmi.bake <- ifelse(pmi.bake < 0, 0, pmi.bake)
ppmi.bake[1:12]

cooking <- as.matrix(cooking)

cooking.exp <- chisq.test(cooking)$expected

cooking.pmi <- log2(cooking/cooking.exp)
cooking.ppmi <- ifelse(cooking.pmi < 0, 0, cooking.pmi)

# cos <- sum(a*b)/(sqrt(sum(a *a)) *sqrt(sum(b *b)))

## Bake(col 1) and Simmer(col 10)

crossprod(cooking.ppmi[, 1], cooking.ppmi[, 10])/sqrt(crossprod(cooking.ppmi[, 1]) *crossprod(cooking.ppmi[, 10]))

## Speed up

cooking1 <- t(cooking.ppmi)
cooking.cos <- cossim(cooking1)
round(cooking.cos, 2)

```


test Levshina 1

```{r}
test_l1 <- c("work", "meet", "help", "let")
```

# Silge & Robinson 2024 [here](https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html)

# Session info

```{r}
sessionInfo()
```
